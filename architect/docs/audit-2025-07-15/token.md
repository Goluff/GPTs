# token.md

## ðŸ§  Token Policy â€” Enforcement and Runtime Safeguards

---

### ðŸ“œ Purpose

This file defines how The Architect handles **context window management**, token overflow risks, and expert coordination under tight budget constraints. It aligns with the 128,000-token limit of GPT-4o and ensures graceful degradation through dynamic compression, user alerts, and modular response strategies.

---

## ðŸ”¢ Context Boundaries

```yaml
max_context_tokens: 128000
```

- The limit is correctly set for GPT-4o.
- This does **not truncate automatically** â€” it defines the theoretical cap against which dynamic strategies are triggered.

---

## ðŸ”„ Segments and Risk Management

The policy is divided into **4 segments**, each addressing a critical operational stage:

---

### ðŸ”¹ Segment 1: `expert_ranking`

**Scope**: Team formation

**Risks**:
- Full expert registry scan may overload token space
- Ambiguity-triggered semantic expansion bloats score phase

**Mitigations**:
- âœ… Tag/domain prefiltering (confirmed present in `architect.yaml`)
- âœ… Warnings if expert pool > 25 (should be reflected in behavior blocks)
- âœ… Batch scoring logic present via `score_experts_by_tags`
- âš ï¸ Only enforced if `expert_score_pool` respects batch boundaries in runtime

---

### ðŸ”¹ Segment 2: `expert_response_integration`

**Scope**: Synthesizing multi-expert replies

**Risks**:
- Large token blocks during consensus/synthesis
- Conflicting replies increase load

**Mitigations**:
- âœ… Show only top-N (80% confidence in `architect.yaml`)
- âœ… Queue others â€” implied by â€œload moreâ€ warning pattern
- âœ… Pause summary â€” reflected in block `rank_responses` logic

---

### ðŸ”¹ Segment 3: `file_snapshot_validation`

**Scope**: Hash validation during file edits

**Risks**:
- If root SHA summary is lost, session cannot verify file integrity

**Mitigations**:
- âœ… Hash summary required (found in `architect.yaml`)
- âœ… Session anchored to `metadata.yaml` hash table
- âœ… Re-archive logic is enforced every 30â€“60 min idle

---

### ðŸ”¹ Segment 4: `user_input_handling`

**Scope**: Logs, configs, or user-pasted code

**Risks**:
- Paste > 8K tokens disrupts team logic
- Experts cannot score or reason under overflow

**Mitigations**:
- âœ… User warned over 8K input
- âœ… Supports `compress_history` override
- âœ… Tasks deferred until input is handled â€” enforced in `behavior_flow`

---

## â±ï¸ Runtime Threshold Actions

### âš ï¸ Nearing Limit â€” 90,000 Tokens

```yaml
runtime_actions:
  nearing_token_limit:
    trigger_at: 90000
    actions:
      - warn_user
      - allow_user_choice
```

âœ… **Confirmed Enforced** in `architect.yaml`:
```yaml
- condition: token_count > 90000
  then:
    respond: "âš ï¸ Context window nearing limit. I may compress earlier content..."
```

---

### ðŸš« Hard Limit Prep â€” 110,000 Tokens

```yaml
runtime_actions:
  overload_detected:
    trigger_at: 110000
    actions:
      - auto_compress_active_team_state
      - suppress logs
      - reanchor with summary_state.yaml
```

âœ… Also enforced:
```yaml
- condition: token_count > 110000
  then:
    compress_state: true
    block_all_actions: true
```

---

## âœ… Audit Summary

| Segment | Risks Addressed | Enforced in `architect.yaml`? |
|---------|------------------|-------------------------------|
| expert_ranking | Token bloating from full scans | âœ… via batch scoring + tag filter |
| expert_response_integration | Reply overflow | âœ… via top-N and synthesis arbitration |
| file_snapshot_validation | SHA mismatch | âœ… via hash snapshots, re-archive prompt |
| user_input_handling | Pasted input overflow | âœ… with chunking, deferral, and override |

| Runtime Event | Actions Present | Enforced? |
|---------------|-----------------|-----------|
| nearing_token_limit | Warn + Choice | âœ… |
| overload_detected | Compress + Suspend | âœ… |

---

### ðŸ§© Final Note

This policy is **fully active** and successfully integrated into your `architect.yaml`. It protects session memory integrity, ensures modular response safety, and prevents expert logic collapse due to token bloat.

